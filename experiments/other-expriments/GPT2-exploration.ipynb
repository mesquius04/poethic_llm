{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HE20qWnsfV9K",
        "outputId": "a805ead1-26f9-43a4-a984-9c0a22c38b6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "#Mount Drive\n",
        "drive.mount('/content/drive/')\n",
        "path = '/content/drive/Shareddrives/DeepLearning/UPF_Deep_Learning_2025/Project/AnotherOurProject'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYzT4xZCiagg",
        "outputId": "5f2e7742-f45a-4463-aa31-75fad141f180"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 239, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 239 (delta 1), reused 0 (delta 0), pack-reused 234 (from 2)\u001b[K\n",
            "Receiving objects: 100% (239/239), 4.38 MiB | 8.23 MiB/s, done.\n",
            "Resolving deltas: 100% (125/125), done.\n",
            "/content/gpt-2\n",
            "Collecting fire>=0.1.3 (from -r requirements.txt (line 1))\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting regex==2017.4.5 (from -r requirements.txt (line 2))\n",
            "  Downloading regex-2017.04.05.tar.gz (601 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.6/601.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting requests==2.21.0 (from -r requirements.txt (line 3))\n",
            "  Downloading requests-2.21.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting tqdm==4.31.1 (from -r requirements.txt (line 4))\n",
            "  Downloading tqdm-4.31.1-py2.py3-none-any.whl.metadata (38 kB)\n",
            "Collecting chardet<3.1.0,>=3.0.2 (from requests==2.21.0->-r requirements.txt (line 3))\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna<2.9,>=2.5 (from requests==2.21.0->-r requirements.txt (line 3))\n",
            "  Downloading idna-2.8-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting urllib3<1.25,>=1.21.1 (from requests==2.21.0->-r requirements.txt (line 3))\n",
            "  Downloading urllib3-1.24.3-py2.py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2025.1.31)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (2.5.0)\n",
            "Downloading requests-2.21.0-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.31.1-py2.py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.3/48.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.8-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.8/118.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: regex, fire\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.4.5-cp311-cp311-linux_x86_64.whl size=667353 sha256=00b2f32b18e6353471c09e3d5cb24e058844780d34272b9ef4457eb2f6e2e65b\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/a6/66/e3a6bac658db8ce49c1537c63759ec8f81150b8bbe3b39deb4\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=100ede396d76f51baa6baba173190a6c2b4b7a6bae32af467795291fc03b1dbc\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built regex fire\n",
            "Installing collected packages: regex, chardet, urllib3, tqdm, idna, fire, requests\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.11.6\n",
            "    Uninstalling regex-2024.11.6:\n",
            "      Successfully uninstalled regex-2024.11.6\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.21.0 which is incompatible.\n",
            "huggingface-hub 0.28.1 requires tqdm>=4.42.1, but you have tqdm 4.31.1 which is incompatible.\n",
            "bigframes 1.38.0 requires requests>=2.27.1, but you have requests 2.21.0 which is incompatible.\n",
            "spacy 3.7.5 requires tqdm<5.0.0,>=4.38.0, but you have tqdm 4.31.1 which is incompatible.\n",
            "yfinance 0.2.54 requires requests>=2.31, but you have requests 2.21.0 which is incompatible.\n",
            "prophet 1.1.6 requires tqdm>=4.36.1, but you have tqdm 4.31.1 which is incompatible.\n",
            "tweepy 4.15.0 requires requests<3,>=2.27.0, but you have requests 2.21.0 which is incompatible.\n",
            "distributed 2024.12.1 requires urllib3>=1.26.5, but you have urllib3 1.24.3 which is incompatible.\n",
            "sentry-sdk 2.22.0 requires urllib3>=1.26.11, but you have urllib3 1.24.3 which is incompatible.\n",
            "dopamine-rl 4.1.2 requires tqdm>=4.64.1, but you have tqdm 4.31.1 which is incompatible.\n",
            "sphinx 8.1.3 requires requests>=2.30.0, but you have requests 2.21.0 which is incompatible.\n",
            "google-genai 1.2.0 requires requests<3.0.0dev,>=2.28.1, but you have requests 2.21.0 which is incompatible.\n",
            "nltk 3.9.1 requires regex>=2021.8.3, but you have regex 2017.4.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 fire-0.7.0 idna-2.8 regex-2017.4.5 requests-2.21.0 tqdm-4.31.1 urllib3-1.24.3\n",
            "Fetching checkpoint: 1.00kit [00:00, 628kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:01, 685kit/s]                                                    \n",
            "Fetching hparams.json: 1.00kit [00:00, 701kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [02:08, 3.89Mit/s]                                  \n",
            "Fetching model.ckpt.index: 6.00kit [00:00, 3.58Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 472kit [00:01, 352kit/s]                                                  \n",
            "Fetching vocab.bpe: 457kit [00:01, 454kit/s]                                                        \n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# 2) Clone the GPT-2 repo & install its requirements\n",
        "!git clone https://github.com/openai/gpt-2.git\n",
        "%cd gpt-2\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# 3) Download the 124M model\n",
        "!python download_model.py 124M\n",
        "\n",
        "# 4) Return to your working directory\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyG-iE2YXye7",
        "outputId": "cd6611db-7764-470e-94ca-44c9bcd5c6a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting regex>=2022.1.18 (from tiktoken)\n",
            "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests>=2.26.0 (from tiktoken)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: requests, regex, tiktoken\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.21.0\n",
            "    Uninstalling requests-2.21.0:\n",
            "      Successfully uninstalled requests-2.21.0\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2017.4.5\n",
            "    Uninstalling regex-2017.4.5:\n",
            "      Successfully uninstalled regex-2017.4.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "huggingface-hub 0.28.1 requires tqdm>=4.42.1, but you have tqdm 4.31.1 which is incompatible.\n",
            "spacy 3.7.5 requires tqdm<5.0.0,>=4.38.0, but you have tqdm 4.31.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed regex-2024.11.6 requests-2.32.3 tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
        "# Source for \"Build a Large Language Model From Scratch\"\n",
        "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
        "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
        "!pip install tiktoken\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tiktoken\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UHvkMXeX1Hz"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
        "# Source for \"Build a Large Language Model From Scratch\"\n",
        "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
        "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class SelfAttention_v1(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = x @ self.W_key\n",
        "        queries = x @ self.W_query\n",
        "        values = x @ self.W_value\n",
        "\n",
        "        attn_scores = queries @ keys.T # omega\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec\n",
        "\n",
        "\n",
        "class SelfAttention_v2(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.T\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec\n",
        "\n",
        "\n",
        "class CausalAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length,\n",
        "                 dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout)  # New\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape  # New batch dimension b\n",
        "        # For inputs where `num_tokens` exceeds `context_length`, this will result in errors\n",
        "        # in the mask creation further below.\n",
        "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs\n",
        "        # do not exceed `context_length` before reaching this forward method.\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(1, 2)  # Changed transpose\n",
        "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
        "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "        attn_weights = self.dropout(attn_weights)  # New\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec\n",
        "\n",
        "\n",
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
        "             for _ in range(num_heads)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)  # optional projection\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "\n",
        "######################\n",
        "# Bonus\n",
        "######################\n",
        "\n",
        "\n",
        "class PyTorchMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, num_heads, dropout=0.0, qkv_bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        assert d_out % num_heads == 0, \"embed_dim is indivisible by num_heads\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "        self.d_out = d_out\n",
        "\n",
        "        self.qkv = nn.Linear(d_in, 3 * d_out, bias=qkv_bias)\n",
        "        self.proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, num_tokens, embed_dim = x.shape\n",
        "\n",
        "        # (b, num_tokens, embed_dim) --> (b, num_tokens, 3 * embed_dim)\n",
        "        qkv = self.qkv(x)\n",
        "\n",
        "        # (b, num_tokens, 3 * embed_dim) --> (b, num_tokens, 3, num_heads, head_dim)\n",
        "        qkv = qkv.view(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n",
        "\n",
        "        # (b, num_tokens, 3, num_heads, head_dim) --> (3, b, num_heads, num_tokens, head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "\n",
        "        # (3, b, num_heads, num_tokens, head_dim) -> 3 times (b, num_heads, num_tokens, head_dim)\n",
        "        queries, keys, values = qkv\n",
        "\n",
        "        use_dropout = 0. if not self.training else self.dropout\n",
        "\n",
        "        context_vec = nn.functional.scaled_dot_product_attention(\n",
        "            queries, keys, values, attn_mask=None, dropout_p=use_dropout, is_causal=True)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.transpose(1, 2).contiguous().view(batch_size, num_tokens, self.d_out)\n",
        "\n",
        "        context_vec = self.proj(context_vec)\n",
        "\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRMevVcWX7g7"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
        "# Source for \"Build a Large Language Model From Scratch\"\n",
        "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
        "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_resid(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed-forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_resid(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest logits value\n",
        "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "######################\n",
        "# Bonus\n",
        "######################\n",
        "\n",
        "\n",
        "class FeedForwardFast(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            nn.GELU(approximate=\"tanh\"),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class TransformerBlockFast(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = PyTorchMultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForwardFast(cfg)\n",
        "        self.norm1 = nn.LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = nn.LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed-forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTModelFast(nn.Module):\n",
        "    \"\"\"\n",
        "    A faster variant of GPTModel optimized for training speed.\n",
        "\n",
        "    This version is only marginally faster on CPU (~1.02x) but significantly\n",
        "    faster on GPU (~2.05x) during training, thanks to optimized CUDA kernels\n",
        "    and FlashAttention support.\n",
        "\n",
        "    Key differences from the original GPTModel:\n",
        "    1. Uses PyTorch's built-in LayerNorm instead of a custom implementation.\n",
        "    2. Uses PyTorch's built-in GELU instead of a custom implementation.\n",
        "    3. Uses PyTorch's scaled_dot_product_attention instead of a custom MultiHeadAttention.\n",
        "    4. Automatically enables FlashAttention on compatible GPUs.\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlockFast(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = nn.LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLCPN52nYA8d"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
        "# Source for \"Build a Large Language Model From Scratch\"\n",
        "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
        "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
        "\n",
        "\n",
        "import json\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward()  # Calculate loss gradients\n",
        "            optimizer.step()  # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n",
        "\n",
        "\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "\n",
        "\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))\n",
        "\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].norm1.scale = assign(\n",
        "            gpt.trf_blocks[b].norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm1.shift = assign(\n",
        "            gpt.trf_blocks[b].norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.trf_blocks[b].norm2.scale = assign(\n",
        "            gpt.trf_blocks[b].norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm2.shift = assign(\n",
        "            gpt.trf_blocks[b].norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
        "\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "\n",
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def download_and_load_gpt2(model_size, models_dir):\n",
        "    import tensorflow as tf\n",
        "    import json # Add import for json\n",
        "\n",
        "    # Validate model size\n",
        "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
        "    if model_size not in allowed_sizes:\n",
        "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
        "\n",
        "    # Define paths\n",
        "    # Use the provided models_dir for the model directory\n",
        "    # Corrected: Use the models_dir argument instead of the global path\n",
        "    model_dir = os.path.join(models_dir, model_size)\n",
        "\n",
        "\n",
        "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
        "    backup_base_url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2\"\n",
        "    filenames = [\n",
        "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
        "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
        "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
        "    ]\n",
        "\n",
        "    # Download files\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    for filename in filenames:\n",
        "        file_url = os.path.join(base_url, model_size, filename)\n",
        "        backup_url = os.path.join(backup_base_url, model_size, filename)\n",
        "        file_path = os.path.join(model_dir, filename)\n",
        "        download_file(file_url, file_path, backup_url)\n",
        "\n",
        "    # Load settings and params\n",
        "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "    settings = json.load(open(os.path.join(model_dir, \"hparams.json\"), \"r\", encoding=\"utf-8\"))\n",
        "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
        "\n",
        "    return settings, params\n",
        "\n",
        "def download_file(url, destination, backup_url=None):\n",
        "    def _attempt_download(download_url):\n",
        "        with urllib.request.urlopen(download_url) as response:\n",
        "            # Get the total file size from headers, defaulting to 0 if not present\n",
        "            file_size = int(response.headers.get(\"Content-Length\", 0))\n",
        "\n",
        "            # Check if file exists and has the same size\n",
        "            if os.path.exists(destination):\n",
        "                file_size_local = os.path.getsize(destination)\n",
        "                if file_size == file_size_local:\n",
        "                    print(f\"File already exists and is up-to-date: {destination}\")\n",
        "                    return True  # Indicate success without re-downloading\n",
        "\n",
        "            block_size = 1024  # 1 Kilobyte\n",
        "\n",
        "            # Initialize the progress bar with total file size\n",
        "            progress_bar_description = os.path.basename(download_url)\n",
        "            with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n",
        "                with open(destination, \"wb\") as file:\n",
        "                    while True:\n",
        "                        chunk = response.read(block_size)\n",
        "                        if not chunk:\n",
        "                            break\n",
        "                        file.write(chunk)\n",
        "                        progress_bar.update(len(chunk))\n",
        "            return True\n",
        "\n",
        "    try:\n",
        "        if _attempt_download(url):\n",
        "            return\n",
        "    except (urllib.error.HTTPError, urllib.error.URLError):\n",
        "        if backup_url is not None:\n",
        "            print(f\"Primary URL ({url}) failed. Attempting backup URL: {backup_url}\")\n",
        "            try:\n",
        "                if _attempt_download(backup_url):\n",
        "                    return\n",
        "            except urllib.error.HTTPError:\n",
        "                pass\n",
        "\n",
        "        # If we reach here, both attempts have failed\n",
        "        error_message = (\n",
        "            f\"Failed to download from both primary URL ({url})\"\n",
        "            f\"{' and backup URL (' + backup_url + ')' if backup_url else ''}.\"\n",
        "            \"\\nCheck your internet connection or the file availability.\\n\"\n",
        "            \"For help, visit: https://github.com/rasbt/LLMs-from-scratch/discussions/273\"\n",
        "        )\n",
        "        print(error_message)\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "\n",
        "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
        "    import tensorflow as tf\n",
        "\n",
        "    # Initialize parameters dictionary with empty blocks for each layer\n",
        "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
        "\n",
        "    # Iterate over each variable in the checkpoint\n",
        "    for name, _ in tf.train.list_variables(ckpt_path):\n",
        "        # Load the variable and remove singleton dimensions\n",
        "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
        "\n",
        "        # Process the variable name to extract relevant parts\n",
        "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
        "\n",
        "        # Identify the target dictionary for the variable\n",
        "        target_dict = params\n",
        "        if variable_name_parts[0].startswith(\"h\"):\n",
        "            layer_number = int(variable_name_parts[0][1:])\n",
        "            target_dict = params[\"blocks\"][layer_number]\n",
        "\n",
        "        # Recursively access or create nested dictionaries\n",
        "        for key in variable_name_parts[1:-1]:\n",
        "            target_dict = target_dict.setdefault(key, {})\n",
        "\n",
        "        # Assign the variable array to the last key\n",
        "        last_key = variable_name_parts[-1]\n",
        "        target_dict[last_key] = variable_array\n",
        "\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw_YJtpfYA-F"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
        "# Source for \"Build a Large Language Model From Scratch\"\n",
        "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
        "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
        "\n",
        "\n",
        "import json\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward()  # Calculate loss gradients\n",
        "            optimizer.step()  # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n",
        "\n",
        "\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "\n",
        "\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))\n",
        "\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].norm1.scale = assign(\n",
        "            gpt.trf_blocks[b].norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm1.shift = assign(\n",
        "            gpt.trf_blocks[b].norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.trf_blocks[b].norm2.scale = assign(\n",
        "            gpt.trf_blocks[b].norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm2.shift = assign(\n",
        "            gpt.trf_blocks[b].norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
        "\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "\n",
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def download_and_load_gpt2(model_size, models_dir):\n",
        "    import tensorflow as tf\n",
        "\n",
        "    # Validate model size\n",
        "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
        "    if model_size not in allowed_sizes:\n",
        "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
        "\n",
        "    # Define paths\n",
        "    model_dir = os.path.join(models_dir, model_size)\n",
        "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
        "    backup_base_url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2\"\n",
        "    filenames = [\n",
        "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
        "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
        "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
        "    ]\n",
        "\n",
        "    # Download files\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    for filename in filenames:\n",
        "        file_url = os.path.join(base_url, model_size, filename)\n",
        "        backup_url = os.path.join(backup_base_url, model_size, filename)\n",
        "        file_path = os.path.join(model_dir, filename)\n",
        "        download_file(file_url, file_path, backup_url)\n",
        "\n",
        "    # Load settings and params\n",
        "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "    settings = json.load(open(os.path.join(model_dir, \"hparams.json\"), \"r\", encoding=\"utf-8\"))\n",
        "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
        "\n",
        "    return settings, params\n",
        "\n",
        "\n",
        "def download_file(url, destination, backup_url=None):\n",
        "    def _attempt_download(download_url):\n",
        "        with urllib.request.urlopen(download_url) as response:\n",
        "            # Get the total file size from headers, defaulting to 0 if not present\n",
        "            file_size = int(response.headers.get(\"Content-Length\", 0))\n",
        "\n",
        "            # Check if file exists and has the same size\n",
        "            if os.path.exists(destination):\n",
        "                file_size_local = os.path.getsize(destination)\n",
        "                if file_size == file_size_local:\n",
        "                    print(f\"File already exists and is up-to-date: {destination}\")\n",
        "                    return True  # Indicate success without re-downloading\n",
        "\n",
        "            block_size = 1024  # 1 Kilobyte\n",
        "\n",
        "            # Initialize the progress bar with total file size\n",
        "            progress_bar_description = os.path.basename(download_url)\n",
        "            with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n",
        "                with open(destination, \"wb\") as file:\n",
        "                    while True:\n",
        "                        chunk = response.read(block_size)\n",
        "                        if not chunk:\n",
        "                            break\n",
        "                        file.write(chunk)\n",
        "                        progress_bar.update(len(chunk))\n",
        "            return True\n",
        "\n",
        "    try:\n",
        "        if _attempt_download(url):\n",
        "            return\n",
        "    except (urllib.error.HTTPError, urllib.error.URLError):\n",
        "        if backup_url is not None:\n",
        "            print(f\"Primary URL ({url}) failed. Attempting backup URL: {backup_url}\")\n",
        "            try:\n",
        "                if _attempt_download(backup_url):\n",
        "                    return\n",
        "            except urllib.error.HTTPError:\n",
        "                pass\n",
        "\n",
        "        # If we reach here, both attempts have failed\n",
        "        error_message = (\n",
        "            f\"Failed to download from both primary URL ({url})\"\n",
        "            f\"{' and backup URL (' + backup_url + ')' if backup_url else ''}.\"\n",
        "            \"\\nCheck your internet connection or the file availability.\\n\"\n",
        "            \"For help, visit: https://github.com/rasbt/LLMs-from-scratch/discussions/273\"\n",
        "        )\n",
        "        print(error_message)\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "\n",
        "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
        "    import tensorflow as tf\n",
        "\n",
        "    # Initialize parameters dictionary with empty blocks for each layer\n",
        "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
        "\n",
        "    # Iterate over each variable in the checkpoint\n",
        "    for name, _ in tf.train.list_variables(ckpt_path):\n",
        "        # Load the variable and remove singleton dimensions\n",
        "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
        "\n",
        "        # Process the variable name to extract relevant parts\n",
        "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
        "\n",
        "        # Identify the target dictionary for the variable\n",
        "        target_dict = params\n",
        "        if variable_name_parts[0].startswith(\"h\"):\n",
        "            layer_number = int(variable_name_parts[0][1:])\n",
        "            target_dict = params[\"blocks\"][layer_number]\n",
        "\n",
        "        # Recursively access or create nested dictionaries\n",
        "        for key in variable_name_parts[1:-1]:\n",
        "            target_dict = target_dict.setdefault(key, {})\n",
        "\n",
        "        # Assign the variable array to the last key\n",
        "        last_key = variable_name_parts[-1]\n",
        "        target_dict[last_key] = variable_array\n",
        "\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sm9eyECZZV5J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZbHDOPbYHAe"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
        "# Source for \"Build a Large Language Model From Scratch\"\n",
        "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
        "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
        "\n",
        "import json\n",
        "import os\n",
        "import psutil\n",
        "import urllib\n",
        "\n",
        "import torch\n",
        "import tqdm\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "def download_and_load_file(file_path, url):\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            text_data = response.read().decode(\"utf-8\")\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(text_data)\n",
        "\n",
        "    # The book originally contained this unnecessary \"else\" clause:\n",
        "    # else:\n",
        "    #     with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    #         text_data = file.read()\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def format_input(entry):\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "    )\n",
        "\n",
        "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
        "\n",
        "    return instruction_text + input_text\n",
        "\n",
        "\n",
        "class InstructionDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "\n",
        "        # Pre-tokenize texts\n",
        "        self.encoded_texts = []\n",
        "        for entry in data:\n",
        "            instruction_plus_input = format_input(entry)\n",
        "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
        "            full_text = instruction_plus_input + response_text\n",
        "            self.encoded_texts.append(\n",
        "                tokenizer.encode(full_text)\n",
        "            )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.encoded_texts[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def custom_collate_draft_1(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # Find the longest sequence in the batch\n",
        "    # and increase the max length by +1, which will add one extra\n",
        "    # padding token below\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Pad and prepare inputs\n",
        "    inputs_lst = []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to batch_max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        # Via padded[:-1], we remove the extra padded token\n",
        "        # that has been added via the +1 setting in batch_max_length\n",
        "        # (the extra padding token will be relevant in later codes)\n",
        "        inputs = torch.tensor(padded[:-1])\n",
        "        inputs_lst.append(inputs)\n",
        "\n",
        "    # Convert list of inputs to tensor and transfer to target device\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    return inputs_tensor\n",
        "\n",
        "\n",
        "def custom_collate_draft_2(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # Find the longest sequence in the batch\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Pad and prepare inputs\n",
        "    inputs_lst, targets_lst = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
        "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    # Convert list of inputs to tensor and transfer to target device\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    targets_tensor = torch.stack(targets_lst).to(device)\n",
        "    return inputs_tensor, targets_tensor\n",
        "\n",
        "\n",
        "def custom_collate_fn(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    ignore_index=-100,\n",
        "    allowed_max_length=None,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # Find the longest sequence in the batch\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Pad and prepare inputs and targets\n",
        "    inputs_lst, targets_lst = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
        "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
        "\n",
        "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
        "        mask = targets == pad_token_id\n",
        "        indices = torch.nonzero(mask).squeeze()\n",
        "        if indices.numel() > 1:\n",
        "            targets[indices[1:]] = ignore_index\n",
        "\n",
        "        # New: Optionally truncate to maximum sequence length\n",
        "        if allowed_max_length is not None:\n",
        "            inputs = inputs[:allowed_max_length]\n",
        "            targets = targets[:allowed_max_length]\n",
        "\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    # Convert list of inputs and targets to tensors and transfer to target device\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    targets_tensor = torch.stack(targets_lst).to(device)\n",
        "\n",
        "    return inputs_tensor, targets_tensor\n",
        "\n",
        "\n",
        "def check_if_running(process_name):\n",
        "    running = False\n",
        "    for proc in psutil.process_iter([\"name\"]):\n",
        "        if process_name in proc.info[\"name\"]:\n",
        "            running = True\n",
        "            break\n",
        "    return running\n",
        "\n",
        "\n",
        "def query_model(\n",
        "    prompt,\n",
        "    model=\"llama3\",\n",
        "    url=\"http://localhost:11434/api/chat\"\n",
        "):\n",
        "    # Create the data payload as a dictionary\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        \"options\": {     # Settings below are required for deterministic responses\n",
        "            \"seed\": 123,\n",
        "            \"temperature\": 0,\n",
        "            \"num_ctx\": 2048\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Convert the dictionary to a JSON formatted string and encode it to bytes\n",
        "    payload = json.dumps(data).encode(\"utf-8\")\n",
        "\n",
        "    # Create a request object, setting the method to POST and adding necessary headers\n",
        "    request = urllib.request.Request(\n",
        "        url,\n",
        "        data=payload,\n",
        "        method=\"POST\"\n",
        "    )\n",
        "    request.add_header(\"Content-Type\", \"application/json\")\n",
        "\n",
        "    # Send the request and capture the response\n",
        "    response_data = \"\"\n",
        "    with urllib.request.urlopen(request) as response:\n",
        "        # Read and decode the response\n",
        "        while True:\n",
        "            line = response.readline().decode(\"utf-8\")\n",
        "            if not line:\n",
        "                break\n",
        "            response_json = json.loads(line)\n",
        "            response_data += response_json[\"message\"][\"content\"]\n",
        "\n",
        "    return response_data\n",
        "\n",
        "\n",
        "def generate_model_scores(json_data, json_key, model=\"llama3\"):\n",
        "    scores = []\n",
        "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
        "        prompt = (\n",
        "            f\"Given the input `{format_input(entry)}` \"\n",
        "            f\"and correct output `{entry['output']}`, \"\n",
        "            f\"score the model response `{entry[json_key]}`\"\n",
        "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
        "            f\"Respond with the integer number only.\"\n",
        "        )\n",
        "        score = query_model(prompt, model)\n",
        "        try:\n",
        "            scores.append(int(score))\n",
        "        except ValueError:\n",
        "            print(f\"Could not convert score: {score}\")\n",
        "            continue\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bA2WB68UaJtL"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
        "# Source for \"Build a Large Language Model From Scratch\"\n",
        "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
        "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
        "\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "import pytest\n",
        "import torch\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize(\"file_name\", [\"the-verdict.txt\"])\n",
        "def test_dataloader(tmp_path, file_name):\n",
        "\n",
        "    if not os.path.exists(\"the-verdict.txt\"):\n",
        "        url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "               \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "               \"the-verdict.txt\")\n",
        "        file_path = \"the-verdict.txt\"\n",
        "        urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "    with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        raw_text = f.read()\n",
        "\n",
        "    vocab_size = 50257\n",
        "    output_dim = 256\n",
        "    context_length = 1024\n",
        "\n",
        "    token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "    pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "\n",
        "    batch_size = 8\n",
        "    max_length = 4\n",
        "    dataloader = create_dataloader_v1(\n",
        "        raw_text,\n",
        "        batch_size=batch_size,\n",
        "        max_length=max_length,\n",
        "        stride=max_length\n",
        "    )\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x, y = batch\n",
        "\n",
        "        token_embeddings = token_embedding_layer(x)\n",
        "        pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "\n",
        "        input_embeddings = token_embeddings + pos_embeddings\n",
        "\n",
        "        break\n",
        "\n",
        "    input_embeddings.shape == torch.Size([8, 4, 256])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiLgkgVraMv_"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
        "# Source for \"Build a Large Language Model From Scratch\"\n",
        "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
        "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "def test_mha():\n",
        "\n",
        "    context_length = 100\n",
        "    d_in = 256\n",
        "    d_out = 16\n",
        "\n",
        "    mha = MultiHeadAttention(d_in, d_out, context_length, dropout=0.0, num_heads=2)\n",
        "\n",
        "    batch = torch.rand(8, 6, d_in)\n",
        "    context_vecs = mha(batch)\n",
        "\n",
        "    context_vecs.shape == torch.Size([8, 6, d_out])\n",
        "\n",
        "    # Test bonus class\n",
        "    mha = PyTorchMultiHeadAttention(d_in, d_out, num_heads=2)\n",
        "\n",
        "    batch = torch.rand(8, 6, d_in)\n",
        "    context_vecs = mha(batch)\n",
        "\n",
        "    context_vecs.shape == torch.Size([8, 6, d_out])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsekaIOyaP7J"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
        "# Source for \"Build a Large Language Model From Scratch\"\n",
        "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
        "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
        "\n",
        "\n",
        "\n",
        "import pytest\n",
        "import torch\n",
        "import tiktoken\n",
        "\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"emb_dim\": 768,          # Embedding dimension\n",
        "    \"n_heads\": 12,           # Number of attention heads\n",
        "    \"n_layers\": 12,          # Number of layers\n",
        "    \"drop_rate\": 0.1,        # Dropout rate\n",
        "    \"qkv_bias\": False        # Query-Key-Value bias\n",
        "}\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize(\"ModelClass\", [GPTModel, GPTModelFast])\n",
        "def test_gpt_model_variants(ModelClass):\n",
        "    torch.manual_seed(123)\n",
        "    model = ModelClass(GPT_CONFIG_124M)\n",
        "    model.eval()  # disable dropout\n",
        "\n",
        "    start_context = \"Hello, I am\"\n",
        "\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    encoded = tokenizer.encode(start_context)\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "\n",
        "    print(f\"\\n{50*'='}\\n{22*' '}IN\\n{50*'='}\")\n",
        "    print(\"\\nInput text:\", start_context)\n",
        "    print(\"Encoded input text:\", encoded)\n",
        "    print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
        "\n",
        "    out = generate_text_simple(\n",
        "        model=model,\n",
        "        idx=encoded_tensor,\n",
        "        max_new_tokens=10,\n",
        "        context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        "    )\n",
        "\n",
        "    expect = torch.tensor([\n",
        "        [15496,   11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267,\n",
        "         49706, 43231, 47062, 34657]\n",
        "    ])\n",
        "    assert torch.equal(expect, out), \"Generated output does not match expected output\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZlVtMGVaRwo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIF6gEbTaRxz"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
        "# Source for \"Build a Large Language Model From Scratch\"\n",
        "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
        "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import urllib\n",
        "\n",
        "import pytest\n",
        "import tiktoken\n",
        "import torch\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 256,  # Shortened for test speed\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": False\n",
        "}\n",
        "\n",
        "OTHER_SETTINGS = {\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"num_epochs\": 2,\n",
        "    \"batch_size\": 1,\n",
        "    \"weight_decay\": 0.1\n",
        "}\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize(\"ModelClass\", [GPTModel, GPTModelFast])\n",
        "def test_train_simple(tmp_path, ModelClass):\n",
        "    torch.manual_seed(123)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    ##############################\n",
        "    # Download data if necessary\n",
        "    ##############################\n",
        "    file_path = tmp_path / \"the-verdict.txt\"\n",
        "    url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            text_data = response.read().decode(\"utf-8\")\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(text_data)\n",
        "    else:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            text_data = f.read()\n",
        "\n",
        "    ##############################\n",
        "    # Set up dataloaders\n",
        "    ##############################\n",
        "    train_ratio = 0.90\n",
        "    split_idx = int(train_ratio * len(text_data))\n",
        "\n",
        "    train_loader = create_dataloader_v1(\n",
        "        text_data[:split_idx],\n",
        "        batch_size=OTHER_SETTINGS[\"batch_size\"],\n",
        "        max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "        stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "        drop_last=True,\n",
        "        shuffle=True,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    val_loader = create_dataloader_v1(\n",
        "        text_data[split_idx:],\n",
        "        batch_size=OTHER_SETTINGS[\"batch_size\"],\n",
        "        max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "        stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "        drop_last=False,\n",
        "        shuffle=False,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    # Limit to 1 batch for speed\n",
        "    train_subset = Subset(train_loader.dataset, range(1))\n",
        "    one_batch_train_loader = DataLoader(train_subset, batch_size=1)\n",
        "    val_subset = Subset(val_loader.dataset, range(1))\n",
        "    one_batch_val_loader = DataLoader(val_subset, batch_size=1)\n",
        "\n",
        "    ##############################\n",
        "    # Train model\n",
        "    ##############################\n",
        "    model = ModelClass(GPT_CONFIG_124M)\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=OTHER_SETTINGS[\"learning_rate\"],\n",
        "        weight_decay=OTHER_SETTINGS[\"weight_decay\"]\n",
        "    )\n",
        "\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "        model, one_batch_train_loader, one_batch_val_loader, optimizer, device,\n",
        "        num_epochs=OTHER_SETTINGS[\"num_epochs\"], eval_freq=1, eval_iter=1,\n",
        "        start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    assert round(train_losses[0], 1) == 7.6\n",
        "    assert round(val_losses[0], 1) == 10.1\n",
        "    assert train_losses[-1] < train_losses[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qKNQcBiaTuT"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
        "# Source for \"Build a Large Language Model From Scratch\"\n",
        "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
        "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
        "\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "import urllib\n",
        "\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "\n",
        "def test_train_classifier(tmp_path):\n",
        "\n",
        "    ########################################\n",
        "    # Download and prepare dataset\n",
        "    ########################################\n",
        "\n",
        "    url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "    zip_path = tmp_path / \"sms_spam_collection.zip\"\n",
        "    extracted_path = tmp_path / \"sms_spam_collection\"\n",
        "    data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
        "\n",
        "    try:\n",
        "        download_and_unzip_spam_data(\n",
        "            url, zip_path, extracted_path, data_file_path\n",
        "        )\n",
        "    except (urllib.error.HTTPError, urllib.error.URLError, TimeoutError) as e:\n",
        "        print(f\"Primary URL failed: {e}. Trying backup URL...\")\n",
        "        backup_url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/sms%2Bspam%2Bcollection.zip\"\n",
        "        download_and_unzip_spam_data(\n",
        "            backup_url, zip_path, extracted_path, data_file_path\n",
        "        )\n",
        "\n",
        "    df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
        "    balanced_df = create_balanced_dataset(df)\n",
        "    balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
        "\n",
        "    train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
        "    train_df.to_csv(tmp_path / \"train.csv\", index=None)\n",
        "    validation_df.to_csv(tmp_path / \"validation.csv\", index=None)\n",
        "    test_df.to_csv(tmp_path / \"test.csv\", index=None)\n",
        "\n",
        "    ########################################\n",
        "    # Create data loaders\n",
        "    ########################################\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    train_dataset = SpamDataset(\n",
        "        csv_file=tmp_path / \"train.csv\",\n",
        "        max_length=None,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    val_dataset = SpamDataset(\n",
        "        csv_file=tmp_path / \"validation.csv\",\n",
        "        max_length=train_dataset.max_length,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    num_workers = 0\n",
        "    batch_size = 8\n",
        "\n",
        "    torch.manual_seed(123)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        dataset=val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    ########################################\n",
        "    # Load pretrained model\n",
        "    ########################################\n",
        "\n",
        "    # Small GPT model for testing purposes\n",
        "    BASE_CONFIG = {\n",
        "        \"vocab_size\": 50257,\n",
        "        \"context_length\": 120,\n",
        "        \"drop_rate\": 0.0,\n",
        "        \"qkv_bias\": False,\n",
        "        \"emb_dim\": 12,\n",
        "        \"n_layers\": 1,\n",
        "        \"n_heads\": 2\n",
        "    }\n",
        "    model = GPTModel(BASE_CONFIG)\n",
        "    model.eval()\n",
        "    device = \"cpu\"\n",
        "\n",
        "    ########################################\n",
        "    # Modify and pretrained model\n",
        "    ########################################\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    torch.manual_seed(123)\n",
        "\n",
        "    num_classes = 2\n",
        "    model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)\n",
        "    model.to(device)\n",
        "\n",
        "    for param in model.trf_blocks[-1].parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    for param in model.final_norm.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    ########################################\n",
        "    # Finetune modified model\n",
        "    ########################################\n",
        "\n",
        "    torch.manual_seed(123)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.0)\n",
        "\n",
        "    train_subset = Subset(train_loader.dataset, range(5))\n",
        "    batch_train_loader = DataLoader(train_subset, batch_size=5)\n",
        "    val_subset = Subset(val_loader.dataset, range(5))\n",
        "    batch_val_loader = DataLoader(val_subset, batch_size=5)\n",
        "\n",
        "    num_epochs = 5\n",
        "    train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
        "        model, batch_train_loader, batch_val_loader, optimizer, device,\n",
        "        num_epochs=num_epochs, eval_freq=1, eval_iter=1,\n",
        "    )\n",
        "\n",
        "    assert round(train_losses[0], 1) == 0.8\n",
        "    assert round(val_losses[0], 1) == 0.8\n",
        "    assert train_losses[-1] < train_losses[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7s8a4HqbaV1u"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
        "# Source for \"Build a Large Language Model From Scratch\"\n",
        "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
        "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
        "\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "import urllib\n",
        "\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "\n",
        "def test_train_classifier(tmp_path):\n",
        "\n",
        "    ########################################\n",
        "    # Download and prepare dataset\n",
        "    ########################################\n",
        "\n",
        "    url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "    zip_path = tmp_path / \"sms_spam_collection.zip\"\n",
        "    extracted_path = tmp_path / \"sms_spam_collection\"\n",
        "    data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
        "\n",
        "    try:\n",
        "        download_and_unzip_spam_data(\n",
        "            url, zip_path, extracted_path, data_file_path\n",
        "        )\n",
        "    except (urllib.error.HTTPError, urllib.error.URLError, TimeoutError) as e:\n",
        "        print(f\"Primary URL failed: {e}. Trying backup URL...\")\n",
        "        backup_url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/sms%2Bspam%2Bcollection.zip\"\n",
        "        download_and_unzip_spam_data(\n",
        "            backup_url, zip_path, extracted_path, data_file_path\n",
        "        )\n",
        "\n",
        "    df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
        "    balanced_df = create_balanced_dataset(df)\n",
        "    balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
        "\n",
        "    train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
        "    train_df.to_csv(tmp_path / \"train.csv\", index=None)\n",
        "    validation_df.to_csv(tmp_path / \"validation.csv\", index=None)\n",
        "    test_df.to_csv(tmp_path / \"test.csv\", index=None)\n",
        "\n",
        "    ########################################\n",
        "    # Create data loaders\n",
        "    ########################################\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    train_dataset = SpamDataset(\n",
        "        csv_file=tmp_path / \"train.csv\",\n",
        "        max_length=None,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    val_dataset = SpamDataset(\n",
        "        csv_file=tmp_path / \"validation.csv\",\n",
        "        max_length=train_dataset.max_length,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    num_workers = 0\n",
        "    batch_size = 8\n",
        "\n",
        "    torch.manual_seed(123)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        dataset=val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    ########################################\n",
        "    # Load pretrained model\n",
        "    ########################################\n",
        "\n",
        "    # Small GPT model for testing purposes\n",
        "    BASE_CONFIG = {\n",
        "        \"vocab_size\": 50257,\n",
        "        \"context_length\": 120,\n",
        "        \"drop_rate\": 0.0,\n",
        "        \"qkv_bias\": False,\n",
        "        \"emb_dim\": 12,\n",
        "        \"n_layers\": 1,\n",
        "        \"n_heads\": 2\n",
        "    }\n",
        "    model = GPTModel(BASE_CONFIG)\n",
        "    model.eval()\n",
        "    device = \"cpu\"\n",
        "\n",
        "    ########################################\n",
        "    # Modify and pretrained model\n",
        "    ########################################\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    torch.manual_seed(123)\n",
        "\n",
        "    num_classes = 2\n",
        "    model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)\n",
        "    model.to(device)\n",
        "\n",
        "    for param in model.trf_blocks[-1].parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    for param in model.final_norm.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    ########################################\n",
        "    # Finetune modified model\n",
        "    ########################################\n",
        "\n",
        "    torch.manual_seed(123)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.0)\n",
        "\n",
        "    train_subset = Subset(train_loader.dataset, range(5))\n",
        "    batch_train_loader = DataLoader(train_subset, batch_size=5)\n",
        "    val_subset = Subset(val_loader.dataset, range(5))\n",
        "    batch_val_loader = DataLoader(val_subset, batch_size=5)\n",
        "\n",
        "    num_epochs = 5\n",
        "    train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
        "        model, batch_train_loader, batch_val_loader, optimizer, device,\n",
        "        num_epochs=num_epochs, eval_freq=1, eval_iter=1,\n",
        "    )\n",
        "\n",
        "    assert round(train_losses[0], 1) == 0.8\n",
        "    assert round(val_losses[0], 1) == 0.8\n",
        "    assert train_losses[-1] < train_losses[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09027MIxaXy_"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
        "# Source for \"Build a Large Language Model From Scratch\"\n",
        "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
        "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
        "\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import tiktoken\n",
        "\n",
        "\n",
        "def test_instruction_finetune(tmp_path):\n",
        "\n",
        "    #######################################\n",
        "    # Download and prepare dataset\n",
        "    #######################################\n",
        "    file_path = tmp_path / \"instruction-data.json\"\n",
        "    url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
        "    data = download_and_load_file(file_path, url)\n",
        "\n",
        "    train_portion = int(len(data) * 0.85)  # 85% for training\n",
        "    test_portion = int(len(data) * 0.1)    # 10% for testing\n",
        "\n",
        "    train_data = data[:train_portion]\n",
        "    test_data = data[train_portion:train_portion + test_portion]\n",
        "    val_data = data[train_portion + test_portion:]\n",
        "\n",
        "    # Use very small subset for testing purposes\n",
        "    train_data = train_data[:15]\n",
        "    val_data = val_data[:15]\n",
        "    test_data = test_data[:15]\n",
        "\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=100)\n",
        "\n",
        "    num_workers = 0\n",
        "    batch_size = 8\n",
        "\n",
        "    torch.manual_seed(123)\n",
        "\n",
        "    train_dataset = InstructionDataset(train_data, tokenizer)\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        collate_fn=customized_collate_fn,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    val_dataset = InstructionDataset(val_data, tokenizer)\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        collate_fn=customized_collate_fn,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    #######################################\n",
        "    # Load pretrained model\n",
        "    #######################################\n",
        "\n",
        "    # Small GPT model for testing purposes\n",
        "    BASE_CONFIG = {\n",
        "        \"vocab_size\": 50257,\n",
        "        \"context_length\": 120,\n",
        "        \"drop_rate\": 0.0,\n",
        "        \"qkv_bias\": False,\n",
        "        \"emb_dim\": 12,\n",
        "        \"n_layers\": 1,\n",
        "        \"n_heads\": 2\n",
        "    }\n",
        "    model = GPTModel(BASE_CONFIG)\n",
        "    model.eval()\n",
        "    device = \"cpu\"\n",
        "    CHOOSE_MODEL = \"Small test model\"\n",
        "\n",
        "    print(\"Loaded model:\", CHOOSE_MODEL)\n",
        "    print(50*\"-\")\n",
        "\n",
        "    #######################################\n",
        "    # Finetuning the model\n",
        "    #######################################\n",
        "\n",
        "    num_epochs = 10\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "\n",
        "    torch.manual_seed(123)\n",
        "    train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "        model, train_loader, val_loader, optimizer, device,\n",
        "        num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "        start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    assert round(train_losses[0], 1) == 10.9\n",
        "    assert round(val_losses[0], 1) == 10.9\n",
        "    assert train_losses[-1] < train_losses[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZVh2CYfZW0_"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
        "# Source for \"Build a Large Language Model From Scratch\"\n",
        "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
        "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class NeuralNetwork(torch.nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = torch.nn.Sequential(\n",
        "\n",
        "            # 1st hidden layer\n",
        "            torch.nn.Linear(num_inputs, 30),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # 2nd hidden layer\n",
        "            torch.nn.Linear(30, 20),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # output layer\n",
        "            torch.nn.Linear(20, num_outputs),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.layers(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "class ToyDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.features = X\n",
        "        self.labels = y\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        one_x = self.features[index]\n",
        "        one_y = self.labels[index]\n",
        "        return one_x, one_y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.labels.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imf5Hb46ZZa5"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
        "# Source for \"Build a Large Language Model From Scratch\"\n",
        "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
        "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
        "\n",
        "\n",
        "import math\n",
        "import torch\n",
        "\n",
        "\n",
        "def find_highest_gradient(model):\n",
        "    max_grad = None\n",
        "    for param in model.parameters():\n",
        "        if param.grad is not None:\n",
        "            grad_values = param.grad.data.flatten()\n",
        "            max_grad_param = grad_values.max()\n",
        "            if max_grad is None or max_grad_param > max_grad:\n",
        "                max_grad = max_grad_param\n",
        "    return max_grad\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, device,\n",
        "                n_epochs, eval_freq, eval_iter, start_context, tokenizer,\n",
        "                warmup_steps, initial_lr=3e-05, min_lr=1e-6, orig_book_version=False):\n",
        "\n",
        "    train_losses, val_losses, track_tokens_seen, track_lrs = [], [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Retrieve the maximum learning rate from the optimizer\n",
        "    peak_lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "    # Calculate the total number of iterations in the training process\n",
        "    total_training_steps = len(train_loader) * n_epochs\n",
        "\n",
        "    # Calculate the learning rate increment during the warmup phase\n",
        "    lr_increment = (peak_lr - initial_lr) / warmup_steps\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "\n",
        "            # Adjust the learning rate based on the current phase (warmup or cosine annealing)\n",
        "            if global_step < warmup_steps:\n",
        "                # Linear warmup\n",
        "                lr = initial_lr + global_step * lr_increment\n",
        "            else:\n",
        "                # Cosine annealing after warmup\n",
        "                progress = ((global_step - warmup_steps) /\n",
        "                            (total_training_steps - warmup_steps))\n",
        "                lr = min_lr + (peak_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "            # Apply the calculated learning rate to the optimizer\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group[\"lr\"] = lr\n",
        "            track_lrs.append(lr)  # Store the current learning rate\n",
        "\n",
        "            # Calculate and backpropagate the loss\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward()\n",
        "\n",
        "            # Apply gradient clipping after the warmup phase to avoid exploding gradients\n",
        "            if orig_book_version:\n",
        "                if global_step > warmup_steps:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            else:\n",
        "                if global_step >= warmup_steps:  # the book originally used global_step > warmup_steps, which lead to a skipped clipping step after warmup\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            tokens_seen += input_batch.numel()\n",
        "\n",
        "            # Periodically evaluate the model on the training and validation sets\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader,\n",
        "                    device, eval_iter\n",
        "                )\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                # Print the current losses\n",
        "                print(f\"Ep {epoch+1} (Iter {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, \"\n",
        "                      f\"Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Generate and print a sample from the model to monitor progress\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen, track_lrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzTfboQMZcxu"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
        "# Source for \"Build a Large Language Model From Scratch\"\n",
        "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
        "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
        "\n",
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "class LoRALayer(torch.nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
        "        super().__init__()\n",
        "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
        "        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))  # similar to standard weight initialization\n",
        "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.alpha * (x @ self.A @ self.B)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LinearWithLoRA(torch.nn.Module):\n",
        "    def __init__(self, linear, rank, alpha):\n",
        "        super().__init__()\n",
        "        self.linear = linear\n",
        "        self.lora = LoRALayer(\n",
        "            linear.in_features, linear.out_features, rank, alpha\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x) + self.lora(x)\n",
        "\n",
        "\n",
        "def replace_linear_with_lora(model, rank, alpha):\n",
        "    for name, module in model.named_children():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            # Replace the Linear layer with LinearWithLoRA\n",
        "            setattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
        "        else:\n",
        "            # Recursively apply the same function to child modules\n",
        "            replace_linear_with_lora(module, rank, alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHK1jTUQZewp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yz6uEsEZexz"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
        "# Source for \"Build a Large Language Model From Scratch\"\n",
        "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
        "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
        "\n",
        "\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import tiktoken\n",
        "\n",
        "\n",
        "def test_instruction_finetune(tmp_path):\n",
        "\n",
        "    #######################################\n",
        "    # Download and prepare dataset\n",
        "    #######################################\n",
        "    file_path = tmp_path / \"instruction-data.json\"\n",
        "    url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
        "    data = download_and_load_file(file_path, url)\n",
        "\n",
        "    train_portion = int(len(data) * 0.85)  # 85% for training\n",
        "    test_portion = int(len(data) * 0.1)    # 10% for testing\n",
        "\n",
        "    train_data = data[:train_portion]\n",
        "    test_data = data[train_portion:train_portion + test_portion]\n",
        "    val_data = data[train_portion + test_portion:]\n",
        "\n",
        "    # Use very small subset for testing purposes\n",
        "    train_data = train_data[:15]\n",
        "    val_data = val_data[:15]\n",
        "    test_data = test_data[:15]\n",
        "\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=100)\n",
        "\n",
        "    num_workers = 0\n",
        "    batch_size = 8\n",
        "\n",
        "    torch.manual_seed(123)\n",
        "\n",
        "    train_dataset = InstructionDataset(train_data, tokenizer)\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        collate_fn=customized_collate_fn,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    val_dataset = InstructionDataset(val_data, tokenizer)\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        collate_fn=customized_collate_fn,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    #######################################\n",
        "    # Load pretrained model\n",
        "    #######################################\n",
        "\n",
        "    # Small GPT model for testing purposes\n",
        "    BASE_CONFIG = {\n",
        "        \"vocab_size\": 50257,\n",
        "        \"context_length\": 120,\n",
        "        \"drop_rate\": 0.0,\n",
        "        \"qkv_bias\": False,\n",
        "        \"emb_dim\": 12,\n",
        "        \"n_layers\": 1,\n",
        "        \"n_heads\": 2\n",
        "    }\n",
        "    model = GPTModel(BASE_CONFIG)\n",
        "    model.eval()\n",
        "    device = \"cpu\"\n",
        "    CHOOSE_MODEL = \"Small test model\"\n",
        "\n",
        "    print(\"Loaded model:\", CHOOSE_MODEL)\n",
        "    print(50*\"-\")\n",
        "\n",
        "    #######################################\n",
        "    # Finetuning the model\n",
        "    #######################################\n",
        "\n",
        "    num_epochs = 10\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "\n",
        "    torch.manual_seed(123)\n",
        "    train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "        model, train_loader, val_loader, optimizer, device,\n",
        "        num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "        start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    assert round(train_losses[0], 1) == 10.9\n",
        "    assert round(val_losses[0], 1) == 10.9\n",
        "    assert train_losses[-1] < train_losses[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UR9qZaGX3Bok",
        "outputId": "d251a819-b3a2-4ba3-923f-27f4a0713aa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "checkpoint    hparams.json\t\t      model.ckpt.index\tvocab.bpe\n",
            "encoder.json  model.ckpt.data-00000-of-00001  model.ckpt.meta\n"
          ]
        }
      ],
      "source": [
        "!ls gpt-2/models/124M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSWUZjhdcvdI",
        "outputId": "eba52826-1d5b-4e5b-cbf9-402cf287a440"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File already exists and is up-to-date: gpt-2/models/124M/checkpoint\n",
            "File already exists and is up-to-date: gpt-2/models/124M/encoder.json\n",
            "File already exists and is up-to-date: gpt-2/models/124M/hparams.json\n",
            "File already exists and is up-to-date: gpt-2/models/124M/model.ckpt.data-00000-of-00001\n",
            "File already exists and is up-to-date: gpt-2/models/124M/model.ckpt.index\n",
            "File already exists and is up-to-date: gpt-2/models/124M/model.ckpt.meta\n",
            "File already exists and is up-to-date: gpt-2/models/124M/vocab.bpe\n",
            "File already exists and is up-to-date: gpt-2/models/124M/checkpoint\n",
            "File already exists and is up-to-date: gpt-2/models/124M/encoder.json\n",
            "File already exists and is up-to-date: gpt-2/models/124M/hparams.json\n",
            "File already exists and is up-to-date: gpt-2/models/124M/model.ckpt.data-00000-of-00001\n",
            "File already exists and is up-to-date: gpt-2/models/124M/model.ckpt.index\n",
            "File already exists and is up-to-date: gpt-2/models/124M/model.ckpt.meta\n",
            "File already exists and is up-to-date: gpt-2/models/124M/vocab.bpe\n",
            "Hello, I am a large language model. I am a language model model for the language model. I am a language model for the model. I am a language model for the model model. I am a language model for the model model. I am a language model for the model model.\n",
            "Hello, I am a large language model. Can you name me?\n",
            "\n",
            "\n",
            "Ana, I am a large number. Can you name me?\n",
            "\n",
            "Ana, I am a number. Can you name?\n",
            "\n",
            "\n",
            "Ana, I am a number. Can you name\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import tiktoken\n",
        "\n",
        "# 1. Define the model configuration\n",
        "# This is an example configuration, you would adjust these values\n",
        "# based on the specific model you want to use or train.\n",
        "cfg = {\n",
        "    \"vocab_size\": 50257,  # Example for GPT-2\n",
        "    \"emb_dim\": 768,       # Example for GPT-2 small\n",
        "    \"context_length\": 1024, # Example for GPT-2\n",
        "    \"n_layers\": 12,       # Example for GPT-2 small\n",
        "    \"n_heads\": 12,        # Example for GPT-2 small\n",
        "    \"drop_rate\": 0.1,     # Example dropout rate\n",
        "    \"qkv_bias\": True     # Example, depends on whether QKV matrices have biases\n",
        "}\n",
        "\n",
        "# 2. Instantiate the model\n",
        "model = GPTModel(cfg)\n",
        "# Or use GPTModelFast for potentially faster training on GPU:\n",
        "#model = GPTModelFast(cfg)\n",
        "\n",
        "# Move model to the appropriate device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# 3. Load pre-trained weights (Optional)\n",
        "# You would need to download the GPT-2 weights first using\n",
        "download_and_load_gpt2(\"124M\", \"gpt-2/models\")\n",
        "# before you can load them. Assuming 'params' contains the loaded weights:\n",
        "settings, params = download_and_load_gpt2(\"124M\", \"gpt-2/models\")\n",
        "load_weights_into_gpt(model, params)\n",
        "\n",
        "# 4. Tokenize input text\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "start_context = \"Hello, I am a large language model.\"\n",
        "encoded_input = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "# 5. Generate text\n",
        "max_new_tokens = 50\n",
        "context_size = cfg[\"context_length\"]\n",
        "generated_token_ids = generate_text_simple(model, encoded_input, max_new_tokens, context_size)\n",
        "\n",
        "# Convert generated token IDs back to text\n",
        "generated_text = token_ids_to_text(generated_token_ids, tokenizer)\n",
        "print(generated_text)\n",
        "\n",
        "# You can also use the more advanced 'generate' function with temperature and top_k\n",
        "generated_token_ids_sampling = generate(\n",
        "     model, encoded_input, max_new_tokens, context_size, temperature=0.7, top_k=50\n",
        " )\n",
        "generated_text_sampling = token_ids_to_text(generated_token_ids_sampling, tokenizer)\n",
        "print(generated_text_sampling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kolAjcPK--ug"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVC0YJrs_I-F",
        "outputId": "d8919d69-c570-45c4-a575-d83dd5e84eed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type 'exit' to stop chatting.\n",
            "\n",
            "GPT: hola, a leader of the Democratic Party of India, said, \"We won't tolerate any kind of intimidation on the part of the BJP, and we expect them to do the right thing.\"\n",
            "\n",
            "The BJP is expected to announce a national strategy for\n",
            "\n",
            "GPT: what to say about him in the comments.<|endoftext|>We have a very good chance to win an interview with you soon.\n",
            "\n",
            "If you're interested in our talks, we have a very good chance to win our book, if you've made it this\n",
            "\n",
            "GPT: how old are you when you die?\"\n",
            "\n",
            "\"Twenty-five,\" replied the man with a smile. \"I'm twenty-five years old and I've been practicing Kung-Tung since I was a kid.\"\n",
            "\n",
            "\"You're very old then,\n",
            "\n",
            "GPT: what are you?\" he muttered to herself. The girl's eyes widened as she looked at him with an expression that looked like she was in shock. \"What are you doing here?\"\n",
            "\n",
            "He turned and started walking towards the door.\n",
            "\n",
            "\"Ah,\n",
            "\n",
            "GPT: how are you?\n",
            "\n",
            "[Sigh] Good morning! I'm sorry. I was hoping that there would be some small change for you. First of all, the house is under siege. However, I have many things to offer you. To help you in\n",
            "\n",
            "GPT: 1+8+11+14+15+18+20+22+24+27+28+29+31+32+33+34+35+36+37+38+39+40+41+42+43+44+45\n",
            "\n",
            "GPT: 2-1\n",
            "\n",
            "Celeste\n",
            "\n",
            "The \"Best Italian Album of 2015\" by Anja\n",
            "\n",
            "The Best Italian Album of the Year is a collection of the best Italian albums of 2015, ranked from 0 to 15. The album is ranked from 1\n",
            "\n",
            "GPT: Messi or Cristiano Ronaldo, it could get more difficult for any player to find his form and form again, especially given the way the season has gone.\n",
            "\n",
            "There's no place for that kind of form in young players. That's what we saw in this season\n"
          ]
        }
      ],
      "source": [
        "def interactive_chat(model, tokenizer, max_new_tokens=50, context_size=1024, temperature=0.7, top_k=50):\n",
        "    model.eval()\n",
        "    print(\"Type 'exit' to stop chatting.\")\n",
        "\n",
        "    while True:\n",
        "        prompt = input(\"\\nYou: \")\n",
        "        if prompt.lower() == 'exit':\n",
        "            print(\"Exiting chat.\")\n",
        "            break\n",
        "\n",
        "        # Tokenize input and move to device\n",
        "        encoded_input = text_to_token_ids(prompt, tokenizer).to(next(model.parameters()).device)\n",
        "\n",
        "\n",
        "        # Generate output\n",
        "        with torch.no_grad():\n",
        "            generated_ids = generate(\n",
        "                model, encoded_input, max_new_tokens, context_size,\n",
        "                temperature=temperature, top_k=top_k\n",
        "            )\n",
        "\n",
        "        # Convert token IDs back to text\n",
        "        response = token_ids_to_text(generated_ids, tokenizer)\n",
        "        print(f\"\\nGPT: {response}\")\n",
        "\n",
        "interactive_chat(model, tokenizer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}